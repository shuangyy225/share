#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-32B LoRAå¾®è°ƒä»£ç  - æ˜‡è…¾910B2ç‰ˆæœ¬
é€‚ç”¨äºï¼š8å¼ æ˜‡è…¾910B2æ˜¾å¡ï¼Œæ¯å¼ 64Gæ˜¾å­˜
ç³»ç»Ÿï¼šopenEuler + MindIEé•œåƒ
"""

# ========================
# ç¬¬ä¸€æ­¥ï¼šå¯¼å…¥å¿…è¦çš„åº“
# ========================
print("æ­£åœ¨å¯¼å…¥å¿…è¦çš„åº“...")

# åŸºç¡€åº“
import os
import json
from typing import Dict, List

# æœºå™¨å­¦ä¹ ç›¸å…³åº“
import torch
from torch.utils.data import Dataset

# HuggingFaceç›¸å…³åº“
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)

# LoRAç›¸å…³åº“ï¼ˆPEFTï¼‰
from peft import LoraConfig, get_peft_model, TaskType

# æ˜‡è…¾ç›¸å…³åº“
import torch_npu  # æ˜‡è…¾NPUæ”¯æŒ
from torch_npu.npu import amp
from torch_npu.contrib import transfer_to_npu  # è‡ªåŠ¨å°†CUDAä»£ç è½¬ä¸ºNPU

print("åº“å¯¼å…¥å®Œæˆï¼")

# ========================
# ç¬¬äºŒæ­¥ï¼šç¯å¢ƒé…ç½®
# ========================
print("æ­£åœ¨é…ç½®æ˜‡è…¾ç¯å¢ƒ...")

# è®¾ç½®æ˜‡è…¾è®¾å¤‡
os.environ['ASCEND_HOME'] = '/usr/local/Ascend'
os.environ['LD_LIBRARY_PATH'] = '/usr/local/Ascend/lib64:' + os.environ.get('LD_LIBRARY_PATH', '')
os.environ['PYTHONPATH'] = '/usr/local/python/site-packages:' + os.environ.get('PYTHONPATH', '')

# è®¾ç½®ä½¿ç”¨æ‰€æœ‰8å¼ NPUå¡
os.environ['RANK'] = '0'
os.environ['WORLD_SIZE'] = '8'
os.environ['MASTER_ADDR'] = '127.0.0.1'
os.environ['MASTER_PORT'] = '29500'

print("æ˜‡è…¾ç¯å¢ƒé…ç½®å®Œæˆï¼")

# ========================
# ç¬¬ä¸‰æ­¥ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®
# ========================
print("æ­£åœ¨å‡†å¤‡è®­ç»ƒæ•°æ®...")

# è¿™é‡Œæ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹æ•°æ®é›†
# ä½ å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹è¿™äº›æ•°æ®
sample_data = [
    {
        "instruction": "ä½ å¥½",
        "output": "ä½ å¥½ï¼æˆ‘æ˜¯Qwenï¼Œé˜¿é‡Œå·´å·´å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹ã€‚å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼"
    },
    {
        "instruction": "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
        "output": "æˆ‘æ˜¯Qwen3ï¼Œæ˜¯é˜¿é‡Œå·´å·´æœ€æ–°æ¨å‡ºçš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘å¯ä»¥å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ã€è¿›è¡Œé€»è¾‘æ¨ç†ç­‰ã€‚"
    },
    {
        "instruction": "ä½ èƒ½åšä»€ä¹ˆï¼Ÿ",
        "output": "æˆ‘å¯ä»¥å¸®åŠ©ä½ å›ç­”é—®é¢˜ã€å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ç­‰ç­‰ï¼Œè¿˜èƒ½è¿›è¡Œé€»è¾‘æ¨ç†å’Œç¼–ç¨‹ã€‚"
    },
    {
        "instruction": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "output": "æˆ‘æ— æ³•è·å–å®æ—¶å¤©æ°”ä¿¡æ¯ã€‚å»ºè®®ä½ æŸ¥çœ‹å¤©æ°”é¢„æŠ¥åº”ç”¨æˆ–ç½‘ç«™è·å–å‡†ç¡®ä¿¡æ¯ã€‚"
    },
    {
        "instruction": "å¸®æˆ‘å†™ä¸€ç¯‡å…³äºæ˜¥å¤©çš„çŸ­æ–‡",
        "output": "æ˜¥å¤©æ˜¯ä¸€å¹´ä¸­æœ€ç¾å¥½çš„å­£èŠ‚ä¹‹ä¸€ã€‚ä¸‡ç‰©å¤è‹ï¼ŒèŠ±å„¿ç»½æ”¾ï¼Œé¸Ÿå„¿æ­Œå”±ã€‚æ¸©æš–çš„é˜³å…‰æ´’åœ¨å¤§åœ°ä¸Šï¼Œç»™äººä»¬å¸¦æ¥å¸Œæœ›å’Œæ´»åŠ›ã€‚"
    }
]

# ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶ï¼ˆæ–¹ä¾¿åç»­ä¿®æ”¹ï¼‰
def save_sample_data():
    """ä¿å­˜ç¤ºä¾‹æ•°æ®åˆ°JSONæ–‡ä»¶"""
    with open('training_data.json', 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)
    print("ç¤ºä¾‹æ•°æ®å·²ä¿å­˜åˆ° training_data.json")

# åŠ è½½æ•°æ®å‡½æ•°
def load_training_data(file_path='training_data.json'):
    """ä»JSONæ–‡ä»¶åŠ è½½è®­ç»ƒæ•°æ®"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"æˆåŠŸåŠ è½½ {len(data)} æ¡è®­ç»ƒæ•°æ®")
        return data
    except FileNotFoundError:
        print("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        save_sample_data()
        return sample_data

# ========================
# ç¬¬å››æ­¥ï¼šåˆ›å»ºæ•°æ®é›†ç±»
# ========================
class QwenDataset(Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ç±»"""

    def __init__(self, data_list: List[Dict], tokenizer, max_length: int = 512):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        :param data_list: æ•°æ®åˆ—è¡¨
        :param tokenizer: åˆ†è¯å™¨
        :param max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.data = data_list
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.data)

    def __getitem__(self, idx):
        """è·å–å•æ¡æ•°æ®"""
        item = self.data[idx]

        # æ„é€ è¾“å…¥æ–‡æœ¬
        text = f"ç”¨æˆ·ï¼š{item['instruction']}\nåŠ©æ‰‹ï¼š{item['output']}"

        # ä½¿ç”¨åˆ†è¯å™¨å¤„ç†æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            truncation=True,           # è¶…é•¿æˆªæ–­
            padding='max_length',      # å¡«å……åˆ°æœ€å¤§é•¿åº¦
            max_length=self.max_length,
            return_tensors='pt'        # è¿”å›PyTorchå¼ é‡
        )

        # è¿”å›è¾“å…¥å’Œæ ‡ç­¾ï¼ˆå› æœè¯­è¨€æ¨¡å‹ä¸­è¾“å…¥å’Œæ ‡ç­¾ç›¸åŒï¼‰
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }

# ========================
# ç¬¬äº”æ­¥ï¼šæ¨¡å‹åŠ è½½å’Œé…ç½®
# ========================
print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")


def load_model_and_tokenizer(model_path="/home/data/model/qwen32B"):
    """
    åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆæœ€ä¸¥æ ¼çš„å…¼å®¹é…ç½®ï¼‰
    """
    print("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        local_files_only=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")

    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map={"": 0},  # å¼ºåˆ¶ä½¿ç”¨å•ä¸ªè®¾å¤‡
            trust_remote_code=True,
            local_files_only=True,
            attn_implementation="eager",
            use_flash_attention_2=False,
            low_cpu_mem_usage=True,
            ignore_mismatched_sizes=True,  # å¿½ç•¥å¤§å°ä¸åŒ¹é…
        )
    except Exception as e:
        print(f"ç¬¬ä¸€æ¬¡åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ: {e}")
        # å¤‡ç”¨åŠ è½½æ–¹æ¡ˆ
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="sequential",
            trust_remote_code=True,
            local_files_only=True,
            attn_implementation="eager",
            use_flash_attention_2=False,
            low_cpu_mem_usage=True,
        )

    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, tokenizer


# ========================
# ç¬¬å…­æ­¥ï¼šé…ç½®LoRAå‚æ•°
# ========================
def setup_lora_config():
    """
    é…ç½®LoRAå¾®è°ƒå‚æ•°
    """
    print("æ­£åœ¨é…ç½®LoRAå‚æ•°...")

    lora_config = LoraConfig(
        r=8,                    # LoRAç§©ï¼ˆè¶Šå°è¶ŠèŠ‚çœèµ„æºï¼‰
        lora_alpha=32,          # LoRAç¼©æ”¾å› å­
        target_modules=[        # æŒ‡å®šè¦åº”ç”¨LoRAçš„æ¨¡å—
            "q_proj",          # æŸ¥è¯¢æŠ•å½±
            "v_proj"           # å€¼æŠ•å½±
        ],
        lora_dropout=0.05,      # Dropoutç‡
        bias="none",            # ä¸è®­ç»ƒåç½®é¡¹
        task_type=TaskType.CAUSAL_LM  # ä»»åŠ¡ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹
    )

    print("LoRAé…ç½®å®Œæˆï¼")
    return lora_config

# ========================
# ç¬¬ä¸ƒæ­¥ï¼šè®­ç»ƒé…ç½®
# ========================
def setup_training_args():
    """
    è®¾ç½®è®­ç»ƒå‚æ•°
    """
    print("æ­£åœ¨è®¾ç½®è®­ç»ƒå‚æ•°...")

    training_args = TrainingArguments(
        # è¾“å‡ºç›®å½•
        output_dir="./qwen3-lora-output",

        # è®­ç»ƒå‚æ•°
        per_device_train_batch_size=1,      # æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°
        gradient_accumulation_steps=8,      # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        num_train_epochs=3,                 # è®­ç»ƒè½®æ•°
        learning_rate=2e-4,                 # å­¦ä¹ ç‡

        # ç¡¬ä»¶ç›¸å…³
        fp16=True,                          # ä½¿ç”¨åŠç²¾åº¦è®­ç»ƒ
        dataloader_pin_memory=False,        # NPUç¯å¢ƒä¸‹å»ºè®®å…³é—­

        # æ—¥å¿—å’Œä¿å­˜
        logging_steps=10,                   # æ—¥å¿—é—´éš”
        save_steps=100,                     # ä¿å­˜é—´éš”
        save_total_limit=2,                 # æœ€å¤šä¿å­˜æ¨¡å‹æ•°é‡

        # å…¶ä»–è®¾ç½®
        report_to="none",                   # ä¸ä½¿ç”¨å¤–éƒ¨æ—¥å¿—å·¥å…·
        remove_unused_columns=False,        # ä¿ç•™æ‰€æœ‰åˆ—

        # æ˜‡è…¾NPUç‰¹å®šè®¾ç½®
        ddp_find_unused_parameters=False,   # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
    )

    print("è®­ç»ƒå‚æ•°è®¾ç½®å®Œæˆï¼")
    return training_args

# ========================
# ç¬¬å…«æ­¥ï¼šä¸»è®­ç»ƒå‡½æ•°
# ========================
def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 50)
    print("ğŸš€ å¼€å§‹Qwen3-32B LoRAå¾®è°ƒè®­ç»ƒ")
    print("=" * 50)

    try:
        # 1. åŠ è½½è®­ç»ƒæ•°æ®
        print("\nğŸ“‹ æ­¥éª¤1ï¼šåŠ è½½è®­ç»ƒæ•°æ®")
        training_data = load_training_data()

        # 2. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        print("\nğŸ¤– æ­¥éª¤2ï¼šåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨")
        model, tokenizer = load_model_and_tokenizer("/home/data/model/qwen32B")

        # 3. åº”ç”¨LoRAé…ç½®
        print("\nğŸ”§ æ­¥éª¤3ï¼šåº”ç”¨LoRAé…ç½®")
        lora_config = setup_lora_config()
        model = get_peft_model(model, lora_config)

        # æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°ä¿¡æ¯
        model.print_trainable_parameters()

        # 4. åˆ›å»ºæ•°æ®é›†
        print("\nğŸ“š æ­¥éª¤4ï¼šåˆ›å»ºè®­ç»ƒæ•°æ®é›†")
        train_dataset = QwenDataset(training_data, tokenizer)

        # 5. è®¾ç½®è®­ç»ƒå‚æ•°
        print("\nâš™ï¸ æ­¥éª¤5ï¼šè®¾ç½®è®­ç»ƒå‚æ•°")
        training_args = setup_training_args()

        # 6. åˆ›å»ºè®­ç»ƒå™¨
        print("\nğŸ‹ï¸ æ­¥éª¤6ï¼šåˆ›å»ºè®­ç»ƒå™¨")
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
            data_collator=DataCollatorForLanguageModeling(
                tokenizer=tokenizer,
                mlm=False  # ä¸ä½¿ç”¨æ©ç è¯­è¨€æ¨¡å‹
            )
        )

        # 7. å¼€å§‹è®­ç»ƒ
        print("\nğŸ”¥ æ­¥éª¤7ï¼šå¼€å§‹è®­ç»ƒ")
        trainer.train()

        # 8. ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print("\nğŸ’¾ æ­¥éª¤8ï¼šä¿å­˜æœ€ç»ˆæ¨¡å‹")
        trainer.save_model("./qwen3-lora-final-model")
        tokenizer.save_pretrained("./qwen3-lora-final-model")

        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° ./qwen3-lora-final-model")

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
        import traceback
        traceback.print_exc()

# ========================
# ç¬¬ä¹æ­¥ï¼šæµ‹è¯•å‡½æ•°
# ========================
def test_model():
    """æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹"""
    print("\nğŸ” æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹")

    try:
        # åŠ è½½å¾®è°ƒåçš„æ¨¡å‹å’Œåˆ†è¯å™¨
        model_path = "./qwen3-lora-final-model"
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        # æµ‹è¯•ç¤ºä¾‹
        test_prompts = [
            "ç”¨æˆ·ï¼šä½ å¥½\nåŠ©æ‰‹ï¼š",
            "ç”¨æˆ·ï¼šä½ èƒ½åšä»€ä¹ˆï¼Ÿ\nåŠ©æ‰‹ï¼š",
            "ç”¨æˆ·ï¼šä»‹ç»ä¸€ä¸‹AI\nåŠ©æ‰‹ï¼š"
        ]

        for prompt in test_prompts:
            print(f"\nè¾“å…¥ï¼š{prompt}")

            # ç¼–ç è¾“å…¥
            inputs = tokenizer(prompt, return_tensors="pt").to("npu")

            # ç”Ÿæˆè¾“å‡º
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.7,
                    do_sample=True
                )

            # è§£ç è¾“å‡º
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"è¾“å‡ºï¼š{result}")

    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")

# ========================
# ç¬¬åæ­¥ï¼šä½¿ç”¨è¯´æ˜
# ========================
def print_usage_instructions():
    """æ‰“å°ä½¿ç”¨è¯´æ˜"""
    print("\n" + "=" * 60)
    print("ğŸ“– ä½¿ç”¨è¯´æ˜")
    print("=" * 60)
    print("1. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼š")
    print("   - ä¿®æ”¹ training_data.json æ–‡ä»¶")
    print("   - æ·»åŠ æ›´å¤šé—®ç­”å¯¹æ¥æé«˜æ¨¡å‹æ•ˆæœ")
    print()
    print("2. è¿è¡Œè®­ç»ƒï¼š")
    print("   python qwen3_lora_finetune.py")
    print()
    print("3. æµ‹è¯•æ¨¡å‹ï¼š")
    print("   python qwen3_lora_finetune.py --test")
    print()
    print("4. æ¨¡å‹ä¿å­˜ä½ç½®ï¼š")
    print("   ./qwen3-lora-final-model/")
    print("=" * 60)

# ========================
# ç¨‹åºå…¥å£ç‚¹
# ========================
if __name__ == "__main__":
    import sys

    # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼
    if "--test" in sys.argv:
        test_model()
    else:
        # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
        print_usage_instructions()

        # ç¡®è®¤å¼€å§‹è®­ç»ƒ
        user_input = input("\næ˜¯å¦å¼€å§‹è®­ç»ƒï¼Ÿ(y/n): ")
        if user_input.lower() in ['y', 'yes', 'æ˜¯']:
            main()
        else:
            print("è®­ç»ƒå·²å–æ¶ˆ")

